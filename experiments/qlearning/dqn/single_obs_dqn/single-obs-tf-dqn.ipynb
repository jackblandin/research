{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T18:14:00.102236Z",
     "start_time": "2019-06-16T18:14:00.091473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    .MathJax_Display { font-size: 23px; }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style>\n",
    "    .MathJax_Display { font-size: 23px; }\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T19:41:03.246Z"
    }
   },
   "source": [
    "$$\n",
    "\\theta = \\theta + \\alpha (r + \\gamma max_{a'} Q(s',a') - Q(s,a)) \\nabla_{\\theta} Q(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T18:14:04.636085Z",
     "start_time": "2019-06-16T18:14:01.779345Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add parent directorys to current path\n",
    "\"\"\"\n",
    "import os.path\n",
    "import sys\n",
    "p = os.path.abspath('..')\n",
    "if p not in sys.path:\n",
    "    sys.path.insert(0,p)\n",
    "p = os.path.abspath('../..')\n",
    "if p not in sys.path:\n",
    "    sys.path.insert(0,p)\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Add tiger-env directory to current path\n",
    "Still not sure why this is needed.\n",
    "\"\"\"\n",
    "p = os.path.abspath('../../../custom_envs/gym-tiger')\n",
    "if p not in sys.path:\n",
    "    sys.path.insert(0, p)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Enable hot-reloading\n",
    "\"\"\"    \n",
    "from notebook_utils import import_module_by_name, reload_module_by_name\n",
    "\n",
    "def reload():\n",
    "    reload_module_by_name('experiments..model', 'DQN')\n",
    "    global DQN\n",
    "    from experiments.dqn.model import DQN\n",
    "    \n",
    "    \n",
    "import gym\n",
    "import gym_tiger\n",
    "import matplotlib.pyplot as plt\n",
    "from experiments.dqn.model import DQN, play_one, main, running_avg, plot_running_avg\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "OBS_GROWL_LEFT = [1, 0, 0, 0]\n",
    "OBS_GROWL_RIGHT = [0, 1, 0, 0]\n",
    "OBS_START = [0, 0, 1, 0]\n",
    "OBS_END = [0, 0, 0, 1]\n",
    "\n",
    "ACTION_OPEN_LEFT = 0\n",
    "ACTION_OPEN_RIGHT = 1\n",
    "ACTION_LISTEN = 2\n",
    "ACTION_MAP = {\n",
    "    ACTION_OPEN_LEFT: 'OPEN_LEFT',\n",
    "    ACTION_OPEN_RIGHT: 'OPEN_RIGHT',\n",
    "    ACTION_LISTEN: 'LISTEN',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play 1000 Episodes with 100% observation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T18:14:31.994205Z",
     "start_time": "2019-06-16T18:14:04.643874Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jblandin/miniconda3/envs/research/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jblandin/miniconda3/envs/research/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "WARNING:tensorflow:From /Users/jblandin/miniconda3/envs/research/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jblandin/miniconda3/envs/research/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jblandin/miniconda3/envs/research/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 total reward: -274 eps: 1.0 avg reward (last 100): -274.0\n",
      "episode: 100 total reward: 1 eps: 0.09950371902099892 avg reward (last 100): -156.92079207920793\n",
      "episode: 200 total reward: -10 eps: 0.07053456158585983 avg reward (last 100): -30.475247524752476\n",
      "episode: 300 total reward: -10 eps: 0.0576390417704235 avg reward (last 100): -23.722772277227723\n",
      "episode: 400 total reward: -10 eps: 0.04993761694389223 avg reward (last 100): -25.574257425742573\n",
      "episode: 500 total reward: -109 eps: 0.04467670516087703 avg reward (last 100): -26.11881188118812\n",
      "episode: 600 total reward: -10 eps: 0.04079085082240021 avg reward (last 100): -23.396039603960396\n",
      "episode: 700 total reward: -10 eps: 0.0377694787300249 avg reward (last 100): -19.03960396039604\n",
      "episode: 800 total reward: -10 eps: 0.03533326266687867 avg reward (last 100): -22.96039603960396\n",
      "episode: 900 total reward: -10 eps: 0.03331483023263848 avg reward (last 100): -21.217821782178216\n",
      "avg reward for last 100 episodes: -15.83\n",
      "total steps: -36697.0\n",
      "               OPEN_LEFT    OPEN_RIGHT     LISTEN\n",
      "-----------  -----------  ------------  ---------\n",
      "START           -31.8723      -25.5862  -0.551066\n",
      "GROWL_LEFT      -38.9804      -30.6945  -0.45058\n",
      "GROWL_RIGHT     -31.6751      -33.0251  -0.35726\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Remove old tensorboard log files\n",
    "\"\"\"\n",
    "!rm -rf ./logs\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "env = gym.make('Tiger-v0')\n",
    "env.__init__(reward_tiger=-100, reward_gold=10, reward_listen=-1,\n",
    "            obs_accuracy=1)\n",
    "gamma = 0.99\n",
    "hidden_layer_sizes = [25, 25]\n",
    "batch_size = 2\n",
    "copy_period = 25\n",
    "N=1000\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "D = env.observation_space.n\n",
    "K = env.action_space.n\n",
    "model = DQN('main', session, D, K, hidden_layer_sizes, gamma,\n",
    "            batch_size=batch_size,\n",
    "            log_summaries=True, logs_dir='./logs')\n",
    "tmodel = DQN('target', session, D, K, hidden_layer_sizes, gamma,\n",
    "             batch_size=batch_size)\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "totalrewards = np.zeros(N)\n",
    "for n in range(N):\n",
    "    eps = 1.0/np.sqrt(n+1)\n",
    "#     eps = 1/(n+1)**(1/5)\n",
    "    totalreward = play_one(env, model, tmodel, eps, gamma,\n",
    "                           copy_period, max_steps=10)\n",
    "    totalrewards[n] = totalreward\n",
    "    if n % 100 == 0:\n",
    "        ravg = running_avg(totalrewards, n)\n",
    "        print('episode:', n,\n",
    "              'total reward:', totalreward,\n",
    "              'eps:', eps,\n",
    "              'avg reward (last 100):', ravg)\n",
    "\n",
    "\n",
    "print('avg reward for last 100 episodes:', totalrewards[-100:].mean())\n",
    "print('total steps:', totalrewards.sum())\n",
    "\n",
    "# plt.plot(totalrewards)\n",
    "# plt.title(\"Rewards\")\n",
    "# plt.show()\n",
    "\n",
    "# plot_running_avg(totalrewards)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Inspect final Q values\n",
    "\"\"\"\n",
    "actions = model.predict([OBS_START,\n",
    "                         OBS_GROWL_LEFT,\n",
    "                         OBS_GROWL_RIGHT])\n",
    "obs = np.array(['START', 'GROWL_LEFT', 'GROWL_RIGHT']).reshape(-1,1)\n",
    "actions = np.hstack([obs, actions])\n",
    "print(tabulate(actions, headers=list(ACTION_MAP.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play 1000 Episodes with 92% observation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T23:35:54.081863Z",
     "start_time": "2019-05-25T23:35:06.907827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 total reward: 10 eps: 1.0 avg reward (last 100): 10.0\n",
      "episode: 100 total reward: 10 eps: 0.09950371902099892 avg reward (last 100): -46.722772277227726\n",
      "episode: 200 total reward: 10 eps: 0.07053456158585983 avg reward (last 100): -40.98019801980198\n",
      "episode: 300 total reward: -100 eps: 0.0576390417704235 avg reward (last 100): -24.465346534653467\n",
      "episode: 400 total reward: -10 eps: 0.04993761694389223 avg reward (last 100): -21.801980198019802\n",
      "episode: 500 total reward: -10 eps: 0.04467670516087703 avg reward (last 100): -18.85148514851485\n",
      "episode: 600 total reward: -10 eps: 0.04079085082240021 avg reward (last 100): -17.99009900990099\n",
      "episode: 700 total reward: -10 eps: 0.0377694787300249 avg reward (last 100): -17.91089108910891\n",
      "episode: 800 total reward: -10 eps: 0.03533326266687867 avg reward (last 100): -19.059405940594058\n",
      "episode: 900 total reward: -10 eps: 0.03331483023263848 avg reward (last 100): -15.564356435643564\n",
      "avg reward for last 100 episodes: -14.1\n",
      "total steps: -23828.0\n",
      "               OPEN_LEFT    OPEN_RIGHT      LISTEN\n",
      "-----------  -----------  ------------  ----------\n",
      "START           -25.8049     -20.6542   -0.0697432\n",
      "GROWL_LEFT      -24.1767      -3.54829   0.67755\n",
      "GROWL_RIGHT     -14.85       -21.3003   -0.708726\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Remove old tensorboard log files\n",
    "\"\"\"\n",
    "!rm -rf ./logs\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "env = gym.make('Tiger-v0')\n",
    "env.__init__(reward_tiger=-100, reward_gold=10, reward_listen=-1,\n",
    "            obs_accuracy=.92)\n",
    "gamma = 0.99\n",
    "hidden_layer_sizes = [25, 25]\n",
    "batch_size = 2\n",
    "copy_period = 25\n",
    "N=1000\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "D = env.observation_space.n\n",
    "K = env.action_space.n\n",
    "model = DQN('main', session, D, K, hidden_layer_sizes, gamma,\n",
    "            batch_size=batch_size,\n",
    "            log_summaries=True, logs_dir='./logs')\n",
    "tmodel = DQN('target', session, D, K, hidden_layer_sizes, gamma,\n",
    "             batch_size=batch_size)\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "totalrewards = np.zeros(N)\n",
    "for n in range(N):\n",
    "    eps = 1.0/np.sqrt(n+1)\n",
    "#     eps = 1/(n+1)**(1/5)\n",
    "    totalreward = play_one(env, model, tmodel, eps, gamma,\n",
    "                           copy_period, max_steps=10)\n",
    "    totalrewards[n] = totalreward\n",
    "    if n % 100 == 0:\n",
    "        ravg = running_avg(totalrewards, n)\n",
    "        print('episode:', n,\n",
    "              'total reward:', totalreward,\n",
    "              'eps:', eps,\n",
    "              'avg reward (last 100):', ravg)\n",
    "\n",
    "\n",
    "print('avg reward for last 100 episodes:', totalrewards[-100:].mean())\n",
    "print('total steps:', totalrewards.sum())\n",
    "\n",
    "# plt.plot(totalrewards)\n",
    "# plt.title(\"Rewards\")\n",
    "# plt.show()\n",
    "\n",
    "# plot_running_avg(totalrewards)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Inspect final Q values\n",
    "\"\"\"\n",
    "actions = model.predict([OBS_START,\n",
    "                         OBS_GROWL_LEFT,\n",
    "                         OBS_GROWL_RIGHT])\n",
    "obs = np.array(['START', 'GROWL_LEFT', 'GROWL_RIGHT']).reshape(-1,1)\n",
    "actions = np.hstack([obs, actions])\n",
    "print(tabulate(actions, headers=list(ACTION_MAP.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Play 1000 Episodes with 85% observation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T23:40:41.241708Z",
     "start_time": "2019-05-25T23:39:44.139551Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 total reward: 8 eps: 1.0 avg reward (last 100): 8.0\n",
      "episode: 100 total reward: -100 eps: 0.09950371902099892 avg reward (last 100): -46.71287128712871\n",
      "episode: 200 total reward: -100 eps: 0.07053456158585983 avg reward (last 100): -46.633663366336634\n",
      "episode: 300 total reward: -10 eps: 0.0576390417704235 avg reward (last 100): -30.0\n",
      "episode: 400 total reward: -10 eps: 0.04993761694389223 avg reward (last 100): -18.415841584158414\n",
      "episode: 500 total reward: -10 eps: 0.04467670516087703 avg reward (last 100): -19.653465346534652\n",
      "episode: 600 total reward: -10 eps: 0.04079085082240021 avg reward (last 100): -19.81188118811881\n",
      "episode: 700 total reward: 7 eps: 0.0377694787300249 avg reward (last 100): -14.049504950495049\n",
      "episode: 800 total reward: -10 eps: 0.03533326266687867 avg reward (last 100): -11.405940594059405\n",
      "episode: 900 total reward: -10 eps: 0.03331483023263848 avg reward (last 100): -21.316831683168317\n",
      "avg reward for last 100 episodes: -19.22\n",
      "total steps: -24697.0\n",
      "               OPEN_LEFT    OPEN_RIGHT    LISTEN\n",
      "-----------  -----------  ------------  --------\n",
      "START           -27.8812      -28.5151   4.26692\n",
      "GROWL_LEFT      -22.8492      -22.6336   3.82962\n",
      "GROWL_RIGHT     -12.3168      -27.3978   4.87747\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Remove old tensorboard log files\n",
    "\"\"\"\n",
    "!rm -rf ./logs\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "env = gym.make('Tiger-v0')\n",
    "env.__init__(reward_tiger=-100, reward_gold=10, reward_listen=-1,\n",
    "            obs_accuracy=.85)\n",
    "gamma = 0.99\n",
    "hidden_layer_sizes = [25, 25]\n",
    "batch_size = 2\n",
    "copy_period = 25\n",
    "N=1000\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "D = env.observation_space.n\n",
    "K = env.action_space.n\n",
    "model = DQN('main', session, D, K, hidden_layer_sizes, gamma,\n",
    "            batch_size=batch_size,\n",
    "            log_summaries=True, logs_dir='./logs')\n",
    "tmodel = DQN('target', session, D, K, hidden_layer_sizes, gamma,\n",
    "             batch_size=batch_size)\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "totalrewards = np.zeros(N)\n",
    "for n in range(N):\n",
    "    eps = 1.0/np.sqrt(n+1)\n",
    "#     eps = 1/(n+1)**(1/5)\n",
    "    totalreward = play_one(env, model, tmodel, eps, gamma,\n",
    "                           copy_period, max_steps=10)\n",
    "    totalrewards[n] = totalreward\n",
    "    if n % 100 == 0:\n",
    "        ravg = running_avg(totalrewards, n)\n",
    "        print('episode:', n,\n",
    "              'total reward:', totalreward,\n",
    "              'eps:', eps,\n",
    "              'avg reward (last 100):', ravg)\n",
    "\n",
    "\n",
    "print('avg reward for last 100 episodes:', totalrewards[-100:].mean())\n",
    "print('total steps:', totalrewards.sum())\n",
    "\n",
    "# plt.plot(totalrewards)\n",
    "# plt.title(\"Rewards\")\n",
    "# plt.show()\n",
    "\n",
    "# plot_running_avg(totalrewards)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Inspect final Q values\n",
    "\"\"\"\n",
    "actions = model.predict([OBS_START,\n",
    "                         OBS_GROWL_LEFT,\n",
    "                         OBS_GROWL_RIGHT])\n",
    "obs = np.array(['START', 'GROWL_LEFT', 'GROWL_RIGHT']).reshape(-1,1)\n",
    "actions = np.hstack([obs, actions])\n",
    "print(tabulate(actions, headers=list(ACTION_MAP.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "145px",
    "left": "1337.666748046875px",
    "right": "20px",
    "top": "121px",
    "width": "351px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
